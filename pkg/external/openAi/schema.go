package openAi

type systemMessage struct {
	// The contents of the system message.
	content string `json:"content"`
	// The role of the messages author, in this case "system".
	role string `json:"role"`
	// An optional name for the participant.
	// Provides the model information to differentiate
	// between participants of the same role.
	name *string `json:"name,omitempty"`
}

type userMessage struct {
	// The contents of the user message.
	content string `json:"content"`
	// The role of the messages author, in this case "user".
	role string `json:"role"`
	// An optional name for the participant.
	// Provides the model information to differentiate
	// between participants of the same role.
	name *string `json:"name,omitempty"`
}

type toolCallFunction struct {
	// The name of the function to call.
	name string `json:"name"`
	// The arguments to call the function with, as generated
	// by the model in JSON format. Note that the model
	// does not always generate valid JSON, and may hallucinate
	// parameters not defined by your function schema. Validate
	// the arguments in your code before calling your function.
	arguments string `json:"arguments"`
}

type toolCall struct {
	// The ID of the tool call.
	id string `json:"id"`
	// The type of the tool. Currently, only "function" is supported.
	typ string `json:"type"`
	// The function that the model called.
	function toolCallFunction `json:"function"`
}

type assistantMessage struct {
	// The contents of the assistant message.
	// Required unless tool_calls is specified.
	content *string `json:"content,omitempty"`
	// The role of the messages author, in this case "assistant".
	role string `json:"role"`
	// An optional name for the participant.
	// Provides the model information to differentiate
	// between participants of the same role.
	name *string `json:"name,omitempty"`
	// The tool calls generated by the model, such as function calls.
	tool_calls []toolCall `json:"tool_calls,omitempty"`
}

type toolMessage struct {
	// The role of the messages author, in this case "tool".
	role string
	// The contents of the tool message.
	content string
	// Tool call that this message is responding to.
	tool_call_id string
}

type Message struct {
	// The contents of the message.
	Content string `json:"content"`
	// The Role of the messages author, can be
	// "system", "user", "assistant", or "tool".
	Role string `json:"role"`
	// An optional Name for the participant.
	// Provides the model information to differentiate
	// between participants of the same role.
	Name *string `json:"name,omitempty"`
	// The tool calls generated by the model, such as function calls.
	Tool_calls *[]toolCall `json:"tool_calls,omitempty"`
}

type ResponseFormat struct {
	// Must be one of text or json_object.
	Typ *string `json:"type,omitempty"`
}

// A list of chat completion choices. Can be more
// than one if n is greater than 1.
type ChatCompletionChoice struct {
	// The reason the model stopped generating tokens.
	// This will be stop if the model hit a natural stop
	// point or a provided stop sequence, length if the
	// maximum number of tokens specified in the request
	// was reached, content_filter if content was omitted
	// due to a flag from our content filters, tool_calls
	// if the model called a tool, or function_call (deprecated)
	// if the model called a function.
	Finish_reason string `json:"finish_reason"`
	// The Index of the choice in the list of choices.
	Index int `json:"index"`
	// A chat completion Message generated by the model.
	Message Message `json:"message"`
}

// Usage statistics for the completion request.
type chatCompletionUsage struct {
	// Number of tokens in the generated completion.
	Completion_tokens int `json:"completion_tokens"`
	// Number of tokens in the prompt.
	Prompt_tokens int `json:"prompt_tokens"`
	// Total number of tokens used in the request (prompt + completion).
	Total_tokens int `json:"total_tokens"`
}

// Represents a chat completion response returned by
// model, based on the provided input.
// (https://platform.openai.com/docs/api-reference/chat/object)
type ChatCompletion struct {
	// A unique identifier for the chat completion.
	ID string `json:"id"`
	//	A list of chat completion Choices. Can be more than one if n is greater than 1.
	Choices []ChatCompletionChoice `json:"choices"`
	// The Unix timestamp (in seconds) of when the chat completion was Created.
	Created int `json:"created"`
	// The Model used for the chat completion.
	Model string `json:"model"`
	// This fingerprint represents the backend configuration that the model runs with.
	// Can be used in conjunction with the seed request parameter to understand
	// when backend changes have been made that might impact determinism.
	System_fingerprint string `json:"system_fingerprint"`
	// The Object type, which is always chat.completion.
	Object string `json:"object"`
	// Usage statistics for the completion request.
	Usage chatCompletionUsage `json:"usage"`
}

// A chat completion delta generated by streamed model responses.
type choiceDelta struct {
	// The contents of the chunk message.
	Content string `json:"content"`
	// The tool calls generated by the model, such as function calls.
	Tool_calls *[]toolCall `json:"tool_calls,omitempty"`
	// The Role of the author of this message.
	Role string `json:"role"`
}

// A list of chat completion choices. Can be more
// than one if n is greater than 1.
type chatCompletionChunkChoice struct {
	// A chat completion Delta generated by streamed model responses.
	Delta choiceDelta `json:"delta"`
	// The reason the model stopped generating tokens.
	// This will be stop if the model hit a natural stop
	// point or a provided stop sequence, length if the
	// maximum number of tokens specified in the request
	// was reached, content_filter if content was omitted
	// due to a flag from our content filters, tool_calls
	// if the model called a tool, or function_call (deprecated)
	// if the model called a function.
	Finish_reason string `json:"finish_reason"`
	// The Index of the choice in the list of choices.
	Index int `json:"index"`
}

type ChatCompletionChunk struct {
	// A unique identifier for the chat completion. Each chunk has the same ID.
	ID string `json:"id"`
	// A list of chat completion Choices. Can be more than one if n is greater than 1.
	Choices []chatCompletionChunkChoice `json:"choices"`
	// The Unix timestamp (in seconds) of when the chat completion was Created.
	Created int `json:"created"`
	// The Model used for the chat completion.
	Model string `json:"model"`
	// This fingerprint represents the backend configuration that the model runs with.
	// Can be used in conjunction with the seed request parameter to understand
	// when backend changes have been made that might impact determinism.
	System_fingerprint string `json:"system_fingerprint"`
	// The Object type, which is always chat.completion.
	Object string `json:"object"`
}

type OpenAiError struct {
	Message string  `json:"message"`
	Type    string  `json:"type"`
	Param   *string `json:"param"`
	Code    *string `json:"code"`
}

func (e *OpenAiError) Error() string {
	return e.Message
}

type OpenAiErrResponse struct {
	Error OpenAiError `json:"error"`
}
